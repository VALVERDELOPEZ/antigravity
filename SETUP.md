# üõ† Lead Finder AI - Gu√≠a de Configuraci√≥n Completa

Esta gu√≠a te ayudar√° a configurar Lead Finder AI para funcionar en **modo 100% real** con **Supabase** y **scraping real** de Reddit, Hacker News e Indie Hackers.

---

## üöÄ Inicio R√°pido (Local)

```powershell
# 1. Activar entorno virtual
.\venv\Scripts\activate

# 2. Instalar dependencias (incluye psycopg2 para Postgres)
pip install -r requirements.txt

# 3. Configurar .env.local con tus credenciales
# Ver secciones abajo

# 4. Inicializar base de datos (Supabase o SQLite)
flask init-db
flask seed-demo

# 5. Iniciar servidor
flask run --port 5000

# 6. Abrir en navegador
# http://localhost:5000
```

---

## üóÑÔ∏è 1. Configuraci√≥n de Supabase (PostgreSQL)

### Obtener URL de conexi√≥n:

1. Ve a [supabase.com](https://supabase.com) y abre tu proyecto
2. Navega a **Settings** ‚Üí **Database**
3. Copia la **Connection string** (URI format)
4. Reemplaza `[YOUR-PASSWORD]` con tu contrase√±a de base de datos

### Variables en `.env.local`:

```env
# Supabase PostgreSQL (formato recomendado)
DATABASE_URL=postgresql+psycopg2://postgres.xxxx:PASSWORD@aws-0-us-east-1.pooler.supabase.com:6543/postgres

# O formato alternativo que Supabase da:
# DATABASE_URL=postgres://postgres.xxxx:PASSWORD@aws-0-us-east-1.pooler.supabase.com:6543/postgres
# (el sistema lo convierte autom√°ticamente a postgresql+psycopg2://)
```

### Inicializar tablas en Supabase:

```powershell
# Con el venv activo y DATABASE_URL configurado
flask init-db

# Esto crear√° las tablas:
# - users
# - leads  
# - transactions
# - automation_logs
```

### Verificar conexi√≥n:

```powershell
# El servidor mostrar√° la URL de conexi√≥n al iniciar
flask run --port 5000

# Deber√≠as ver algo como:
# Using database: postgresql+psycopg2://postgres.xxx@...
```

---

## üìß 2. Configuraci√≥n de Gmail SMTP (Env√≠o de Emails Real)

Para que la app env√≠e correos electr√≥nicos reales:

### Pasos:
1. **Seguridad de Google:** Ve a tu [Cuenta de Google](https://myaccount.google.com/security).
2. **2FA:** Aseg√∫rate de que la "Verificaci√≥n en dos pasos" est√© **Activada**.
3. **Contrase√±a de Aplicaci√≥n:**
   - Busca "Contrase√±as de aplicaciones" en la barra de b√∫squeda superior.
   - Crea una nueva llamada "Lead Finder AI".
   - Google te dar√° una clave de **16 caracteres** (ej: `abcd efgh ijkl mnop`).

### Variables en `.env.local`:
```env
SMTP_SERVER=smtp.gmail.com
SMTP_PORT=587
SMTP_USERNAME=tu-correo@gmail.com
SMTP_PASSWORD=abcdefghijklmnop
EMAIL_FROM_NAME=Lead Finder AI
EMAIL_FROM_ADDRESS=tu-correo@gmail.com
```

### Verificar:
- Si SMTP est√° configurado ‚Üí Los emails se env√≠an realmente
- Si NO est√° configurado ‚Üí Los emails se guardan en `mail_simulation.log`

---

## üí≥ 3. Configuraci√≥n de Stripe (Modo Test)

Para activar los pagos y suscripciones:

### Pasos:
1. **API Keys:** Obt√©n tus claves en [Stripe Developers](https://dashboard.stripe.com/test/apikeys).
   - `STRIPE_PUBLISHABLE_KEY` (empieza por `pk_test_`)
   - `STRIPE_SECRET_KEY` (empieza por `sk_test_`)

2. **Productos:** Ve a [Productos](https://dashboard.stripe.com/test/products) y crea dos:
   - **Starter:** 49.00 EUR / Mensual
   - **Pro:** 99.00 EUR / Mensual

3. **Price IDs:** Copia el "ID de API" de cada precio (empieza por `price_`).

4. **Webhook:** Ve a [Webhooks](https://dashboard.stripe.com/test/webhooks).
   - Endpoint: `https://tu-url.render.com/webhook/stripe`
   - Eventos: `checkout.session.completed`, `customer.subscription.updated`, `customer.subscription.deleted`
   - Copia el "Secreto de firma" (`whsec_...`)

### Variables en `.env.local`:
```env
STRIPE_SECRET_KEY=sk_test_...
STRIPE_PUBLISHABLE_KEY=pk_test_...
STRIPE_PRICE_STARTER=price_...
STRIPE_PRICE_PRO=price_...
STRIPE_WEBHOOK_SECRET=whsec_...
```

---

## üîç 4. Scraping REAL (Reddit, HN, Indie Hackers)

### Estrategia de Scraping para MVP

> **Objetivo:** Validar el mercado con coste $0 en APIs externas para conseguir los primeros 10-50 clientes.

#### ¬øPor qu√© usamos endpoints JSON en lugar de la API oficial de Reddit?

| Aspecto | JSON Endpoints | API Oficial (PRAW) |
|---------|---------------|-------------------|
| **Coste** | $0 | $0 (free tier limitado) |
| **OAuth requerido** | ‚ùå No | ‚úÖ S√≠ |
| **Rate limits** | ~60 req/min | 60 req/min |
| **Setup** | Ninguno | Registrar app |
| **Datos disponibles** | Posts, scores, comments | Posts, scores, comments, m√°s |
| **Riesgo** | Bloqueo si abusamos | Bajo |

#### Fuentes implementadas:

1. **Reddit (JSON endpoints)**
   - URL: `https://www.reddit.com/r/{subreddit}/search.json?q={query}`
   - Datos: t√≠tulo, contenido, score, comentarios, autor
   - Rate limiting: 3-8 segundos entre requests (jitter)
   - User-Agent: rotaci√≥n autom√°tica

2. **Hacker News**
   - Show HN: personas lanzando proyectos (potenciales clientes)
   - Ask HN: personas con preguntas/problemas (pain points)
   - API Firebase + BeautifulSoup

3. **Indie Hackers**
   - Feed principal con BeautifulSoup
   - Emprendedores en fase temprana

#### Protecciones implementadas:

- ‚úÖ Rotaci√≥n de User-Agent (6 navegadores diferentes)
- ‚úÖ Jitter aleatorio (3-8 segundos entre requests)
- ‚úÖ M√°ximo de requests por ciclo (configurable, default 20)
- ‚úÖ Filtro de engagement m√≠nimo (evita spam/bots)
- ‚úÖ Deduplicaci√≥n por external_id

### Variables de Scraping en `.env.local`:

```env
# Intervalo del scheduler (minutos)
SCRAPE_INTERVAL_MINUTES=30

# Engagement m√≠nimo para guardar un lead (upvotes + comments)
MIN_ENGAGEMENT_SCORE=2

# M√°ximo requests por ciclo (rate limiting)
MAX_REQUESTS_PER_CYCLE=20
```

### Probar Scraping Manualmente:

```powershell
# Test del scraper standalone
python automation/scraper.py

# Ejecutar un ciclo del scheduler
python automation/scheduler.py --once

# Ver leads reales en la BD
sqlite3 instance/leadfinder.db "SELECT platform, source, title FROM leads WHERE source_type='real' LIMIT 10;"

# O en Supabase (desde el dashboard SQL Editor):
# SELECT platform, source, title FROM leads WHERE source_type='real' LIMIT 10;
---

## üåê 5. Sistema Multi-Idioma (EN, ES, PT, FR)

El sistema soporta leads en **4 idiomas** desde d√≠a 1:

### Idiomas Configurados

| Idioma | C√≥digo | Bandera | Subreddits | Keywords |
|--------|--------|---------|------------|----------|
| **English** | `en` | üá¨üáß | Entrepreneur, startups, SaaS, etc. | cold email, lead generation, etc. |
| **Espa√±ol** | `es` | üá™üá∏ | Emprendedores, mexico, Colombia, etc. | email fr√≠o, necesito clientes, etc. |
| **Portugu√™s** | `pt` | üáßüá∑ | Empreendedorismo, brasil, etc. | gera√ß√£o de leads, preciso clientes, etc. |
| **Fran√ßais** | `fr` | üá´üá∑ | Entrepreneur_FR, France, etc. | g√©n√©ration de leads, besoin clients, etc. |

### C√≥mo Funciona

1. **Scraping Multi-Idioma:**
   - El scheduler scrapea todos los idiomas en paralelo
   - Cada lead se etiqueta con su idioma (`lead.language`)
   - Subreddits y keywords espec√≠ficos por idioma

2. **Dashboard:**
   - Columna "Idioma" con bandera
   - Filtro por idioma en la navegaci√≥n
   - Estad√≠sticas por idioma

3. **Generaci√≥n de Emails:**
   - IA detecta el idioma del lead
   - Email se genera EN EL MISMO IDIOMA del post original
   - Sistema biling√ºe para todos los idiomas

4. **Scoring con IA:**
   - An√°lisis considera el idioma
   - Res√∫menes en el idioma del lead

### Configuraci√≥n de Subreddits por Idioma

```python
# automation/scraper.py

SUBREDDITS_BY_LANGUAGE = {
    "en": ["Entrepreneur", "startups", "smallbusiness", "GrowthHacking", "SaaS"],
    "es": ["Emprendedores", "mexico", "Colombia", "argentina", "es"],
    "pt": ["Empreendedorismo", "brasil", "investimentos", "brdev"],
    "fr": ["Entrepreneur_FR", "France", "besoindeparler", "vosfinances"]
}

KEYWORDS_BY_LANGUAGE = {
    "en": ["cold email", "lead generation", "B2B growth", "need clients"],
    "es": ["email fr√≠o", "necesito clientes", "busco clientes", "crecimiento B2B"],
    "pt": ["gera√ß√£o de leads", "preciso clientes", "email frio"],
    "fr": ["g√©n√©ration de leads", "besoin clients", "email froid"]
}
```

### A√±adir un Nuevo Idioma

Para agregar un nuevo idioma (ej: italiano):

1. **Modificar `automation/scraper.py`:**
   ```python
   SUBREDDITS_BY_LANGUAGE["it"] = ["italy", "startup_italia", "imprenditorialita"]
   KEYWORDS_BY_LANGUAGE["it"] = ["generazione lead", "bisogno clienti"]
   LANGUAGE_INFO["it"] = {"name": "Italiano", "flag": "üáÆüáπ", "display": "IT"}
   ```

2. **Actualizar la migraci√≥n de BD:**
   ```sql
   -- El campo language ya soporta cualquier c√≥digo ISO de 2-5 caracteres
   -- No se necesita migraci√≥n adicional
   ```

3. **Actualizar el dashboard (`templates/dashboard/leads.html`):**
   ```html
   <option value="it">üáÆüáπ Italiano</option>
   
   {% elif lead.language == 'it' %}
   üáÆüáπ IT
   ```

4. **Actualizar `automation/ai_generator.py`:**
   ```python
   language_config['it'] = {
       'instruction': "Scrivi l'email IN ITALIANO.",
       'system': "Sei un esperto nella scrittura di email a freddo."
   }
   ```

### Verificar Multi-Idioma

```powershell
# Ejecutar scraping multi-idioma
python automation/scheduler.py --once

# Ver leads por idioma en Supabase:
# SELECT language, COUNT(*) FROM leads GROUP BY language;

# Ver en el dashboard: /dashboard/leads
# Usar el filtro de idioma para ver leads espec√≠ficos
```

---

## ü§ñ 6. Scheduler de Automatizaci√≥n

El scheduler busca leads autom√°ticamente cada X minutos.

### Ejecutar en Local:

```powershell
# En una terminal separada (con venv activo)
python automation/scheduler.py

# O ejecutar solo un ciclo (para testing):
python automation/scheduler.py --once
```

### Variables de Control:

```env
# Para MVP (actualizaciones frecuentes)
SCRAPE_INTERVAL_MINUTES=30

# Para producci√≥n (menos agresivo)
SCRAPE_INTERVAL_HOURS=8
```

### En Render (Background Worker):

1. Crea un nuevo **Background Worker** en Render
2. Configuraci√≥n:
   - **Build Command:** `pip install -r requirements-prod.txt`
   - **Start Command:** `python automation/scheduler.py`
3. A√±ade las mismas variables de entorno que el Web Service

---

## üè≠ 7. Modo Producci√≥n (100% Real) - CHECKLIST

### Requisitos M√≠nimos:

| Componente | Variable | Obligatorio |
|------------|----------|-------------|
| Supabase (DB) | `DATABASE_URL` | ‚úÖ S√≠ |
| OpenAI (Scoring) | `OPENAI_API_KEY` | ‚úÖ S√≠ |
| Gmail (Emails) | `SMTP_*` variables | ‚ö° Recomendado |
| Stripe (Pagos) | `STRIPE_*` variables | ‚ö° Recomendado |

### Checklist de Producci√≥n:

- [ ] `.env.local` tiene `DATABASE_URL` de Supabase (PostgreSQL)
- [ ] Ejecut√© `flask init-db` y las tablas se crearon en Supabase
- [ ] `.env.local` tiene `OPENAI_API_KEY` real
- [ ] `.env.local` tiene configuraci√≥n SMTP completa
- [ ] Prob√© el scraper: `python automation/scraper.py` funciona
- [ ] Los leads en el dashboard muestran la fuente real:
  - `Reddit (Entrepreneur)`, `Reddit (SaaS)`, etc.
  - `HN (Show HN)`, `HN (Ask HN)`
  - `IndieH (real)`
- [ ] El scheduler est√° corriendo: `python automation/scheduler.py`
- [ ] El scheduler genera leads reales cada 30 minutos
- [ ] (Opcional) Stripe est√° configurado y los pagos funcionan en modo test

### C√≥mo Saber si Est√° en Modo Real:

1. **En el Dashboard de Leads:** La columna "Fuente" muestra:
   - üü¢ **Reddit (subreddit)** = Scraped real de Reddit
   - üü¢ **HN (Show HN)** = Scraped real de Hacker News
   - üü¢ **IndieH (real)** = Scraped real de Indie Hackers
   - üü£ **AI** = Generado por IA (fallback)
   - ‚ö™ **Demo** = Datos de prueba

2. **En logs del scheduler:**
   ```
   LEAD FINDER AI - REAL SCRAPING SCHEDULER
   Mode: PRODUCTION (Real Scraping)
   Sources: Reddit JSON, Hacker News, Indie Hackers
   ```

---

## ‚úÖ 8. Checklist Final de Validaci√≥n

### Autenticaci√≥n:
- [ ] Puedo registrarme en `/signup`
- [ ] Puedo loguearme con `demo@leadfinderai.com` / `demo123456`
- [ ] Veo el dashboard con estad√≠sticas

### Base de Datos:
- [ ] La app usa PostgreSQL (Supabase) en producci√≥n
- [ ] Las tablas existen: `users`, `leads`, `automation_logs`
- [ ] Puedo ver datos en el dashboard de Supabase

### Leads:
- [ ] Ejecuto `python automation/scheduler.py --once` sin errores
- [ ] Veo los nuevos leads en `/dashboard/leads`
- [ ] Los leads REALES muestran la fuente espec√≠fica (Reddit/HN/IndieH)
- [ ] El campo `source_type` es `real` para leads scrapeados

### Emails:
- [ ] Click en el icono de sobre para generar email
- [ ] El email se genera con IA (personalizado al lead)
- [ ] Con SMTP configurado, el email se env√≠a realmente
- [ ] Sin SMTP, el email aparece en `mail_simulation.log`

### Pagos (Opcional):
- [ ] Click "Upgrade" abre Stripe Checkout
- [ ] Pago con tarjeta test `4242 4242 4242 4242` ‚Üí Success
- [ ] El plan se actualiza en Settings

### Scheduler:
- [ ] `python automation/scheduler.py` inicia sin errores
- [ ] Muestra "REAL SCRAPING SCHEDULER"
- [ ] Genera leads cada 30 minutos (configurable)

---

## üîß Comandos √ötiles

```powershell
# Activar entorno
.\venv\Scripts\activate

# Iniciar servidor
flask run --port 5000

# Ejecutar un ciclo de scraping (testing)
python automation/scheduler.py --once

# Iniciar scheduler continuo
python automation/scheduler.py

# Test standalone del scraper
python automation/scraper.py

# Inicializar BD en Supabase
flask init-db

# Crear usuario demo
flask seed-demo

# Ver variables de entorno cargadas
python -c "from dotenv import load_dotenv; load_dotenv('.env.local'); import os; print(os.getenv('DATABASE_URL')[:50])"
```

---

## üìÅ Archivos de Configuraci√≥n

| Archivo | Prop√≥sito |
|---------|-----------|
| `.env.local` | Variables de entorno (TUS claves) |
| `.env.example` | Plantilla con todas las variables |
| `config.py` | Configuraci√≥n de la app (lee de .env) |
| `render.yaml` | Blueprint para deploy en Render |
| `requirements.txt` | Dependencias de desarrollo |
| `requirements-prod.txt` | Dependencias de producci√≥n |

---

## üóÑÔ∏è Esquema de Base de Datos

### Tabla `users`
```sql
id, email, password_hash, name, plan, stripe_customer_id, 
keywords, platforms, leads_found_count, emails_sent_count, ...
```

### Tabla `leads`
```sql
id, user_id, username, platform, title, content, post_url,
external_id,  -- ID del post en la plataforma origen
source,       -- "r/Entrepreneur", "Show HN", etc.
source_type,  -- 'real', 'ai_generated', 'demo'
score, urgency, status, email_sent, ...
```

### Tabla `automation_logs`
```sql
id, event_type, platform, status, leads_found, 
message, error_message, created_at, ...
```

---

## üö® Migraci√≥n de SQLite a Supabase

Si ya tienes datos en SQLite y quieres migrarlos a Supabase:

```python
# Script de migraci√≥n (ejecutar una vez)
# migrate_to_supabase.py

import sqlite3
import os
from dotenv import load_dotenv
load_dotenv('.env.local')

# 1. Exportar de SQLite
conn = sqlite3.connect('instance/leadfinder.db')
# ... exportar a CSV o usar pg_dump

# 2. Importar a Supabase
# Usar el SQL Editor de Supabase o herramientas como DBeaver
```

Para proyectos nuevos, simplemente configura `DATABASE_URL` de Supabase y ejecuta `flask init-db`.

---

## üìä M√©tricas de Rendimiento Esperadas

| M√©trica | Valor Esperado |
|---------|---------------|
| Leads por ciclo | 5-20 |
| Tiempo por ciclo | 2-5 minutos |
| Requests por ciclo | 10-20 |
| Leads duplicados | 0 (deduplicaci√≥n activa) |
| Tasa de leads "hot" (8+) | 20-30% |

---

*√öltima actualizaci√≥n: Enero 2026*
